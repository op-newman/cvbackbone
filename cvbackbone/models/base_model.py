# file description:
# This file contains the base model class. All models should inherit from this class.
# ğŸš€the file base_model.py has been created by PB on 2025/05/20 15:13:44


import torch.nn as nn
import torch
from typing import Dict, Tuple

class BaseModel(nn.Module):
    """
    æ‰€æœ‰æ¨¡å‹çš„åŸºç±»ï¼Œå®šä¹‰æ ‡å‡†æ¥å£å’ŒåŸºç¡€åŠŸèƒ½
    ç»§æ‰¿è‡ªè¯¥ç±»çš„æ¨¡å‹éœ€è¦å®ç°ä»¥ä¸‹æ–¹æ³•ï¼š
    1. forward(): å®šä¹‰å‰å‘ä¼ æ’­
    2. get_input_size(): è¿”å›æ¨¡å‹æœŸæœ›çš„è¾“å…¥å°ºå¯¸
    """
    
    def __init__(self):
        super().__init__()
        self._initialized = False
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å¿…é¡»å®ç°çš„å‰å‘ä¼ æ’­é€»è¾‘
        Args:
            x: è¾“å…¥å¼ é‡ (B, C, H, W)
        Returns:
            è¾“å‡ºå¼ é‡ (B, num_classes)
        """
        raise NotImplementedError("All models must implement forward()")
    
    def get_input_size(self) -> Tuple[int, int]:
        """
        è¿”å›æ¨¡å‹æœŸæœ›çš„è¾“å…¥å°ºå¯¸ (H, W)
        ç”¨äºæ•°æ®é¢„å¤„ç†å’ŒéªŒè¯
        """
        raise NotImplementedError("All models must implement get_input_size()")
    
    def init_weights(self, init_cfg: Dict = None):
        """
        æƒé‡åˆå§‹åŒ– (å¯é€‰å®ç°)
        Args:
            init_cfg: åˆå§‹åŒ–é…ç½®å­—å…¸ï¼Œä¾‹å¦‚:
                {
                    'type': 'kaiming',  # å¯é€‰é¡¹: ['normal', 'xavier', 'kaiming', 'pretrained']
                    'mode': 'fan_out',
                    'pretrained_path': ''
                }
        """
        if init_cfg is None:
            return
            
        if init_cfg['type'] == 'pretrained':
            self.load_state_dict(torch.load(init_cfg['pretrained_path']))
        else:
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    if init_cfg['type'] == 'kaiming':
                        nn.init.kaiming_normal_(
                            m.weight, 
                            mode=init_cfg.get('mode', 'fan_out'),
                            nonlinearity='relu'
                        )
                    elif init_cfg['type'] == 'xavier':
                        nn.init.xavier_normal_(m.weight)
                    elif init_cfg['type'] == 'normal':
                        nn.init.normal_(m.weight, std=0.01)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.BatchNorm2d):
                    nn.init.constant_(m.weight, 1)
                    nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.Linear):
                    nn.init.normal_(m.weight, std=0.01)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
        self._initialized = True
    
    def get_num_params(self, trainable: bool = True) -> int:
        """
        è®¡ç®—æ¨¡å‹å‚æ•°é‡
        Args:
            trainable: æ˜¯å¦åªè®¡ç®—å¯è®­ç»ƒå‚æ•°
        Returns:
            å‚æ•°æ€»é‡
        """
        return sum(p.numel() for p in self.parameters() if not trainable or p.requires_grad)
    
    def get_flops(self, input_size: Tuple[int, int] = None) -> float:
        """
        ä¼°ç®—æ¨¡å‹FLOPs (éœ€è¦å®ç°å…·ä½“é€»è¾‘)
        Args:
            input_size: è¾“å…¥å°ºå¯¸ (H, W)
        Returns:
            æµ®ç‚¹è¿ç®—æ¬¡æ•° (FLOPs)
        """
        raise NotImplementedError("FLOPs calculation not implemented")
        # å®é™…å®ç°å¯ä»¥ä½¿ç”¨thopç­‰åº“
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹æƒé‡"""
        torch.save(self.state_dict(), path)
    
    def load(self, path: str, strict: bool = True):
        """åŠ è½½æ¨¡å‹æƒé‡"""
        self.load_state_dict(torch.load(path), strict=strict)